{"/":{"title":"Prachee Nanda","content":"Hi there üëã! Welcome to my digital garden, a collection of notes on topics that have captured my interest and curiosity. Inspired by the Zettelkasten approach, my aim is to expand my knowledge and discover new insights.\n\nThis notebook will keep on changing and improving over time, so don't expect it to be perfect yet. Some pages might still be a work in progress.\n- [[notes/Projects|Projects]]\n- [[notes/Areas|Areas]]\n- [[notes/Resources|Resources]]\n- [[notes/Archives|Archives]]\n\nWhile I'm still playing around with how I want to build my `second brain`, here are some basic principles I follow!\n\nMy process for generating content for the second brain is based on the CODE system. This involves 4 steps: **C**apture, **O**rganize, **D**istill and **E**xpress. In brief,\n- Capture: The idea is to capture information that is interesting, relevant or personal to me.\n- Organize: I plan to write on a variety of topics, so organizing all these notes is key! The basic rule of thumb is PARA, which stands for **PROJECTS**, **AREAS**, **RESOURCES**, **ARCHIVE**. I'm also looking into tagging all notes as an alternative form of finding content in the future.\n- Distill: Condensing information into the key points. This would be more critical when I am taking course notes or self-learning a topic.\n- Express: With Obsidian, I want to use the Graph view to make connections across the various notes.\n","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Alpha-beta-Pruning":{"title":"Alpha-beta Pruning","content":"Search algorithm that decreases the number of gate states to evaluate in [[Braitenberg vehicles|Braitenberg vehicles]] in the game tree.\n- Stopping condition: It does not evaluate further when it realizes that there is already an existing better alternative.\nNote: It's the same final output as Minimax, just a more efficient way of going about it.\n\n##### High-level Overview\nLike minimax,\n- Each node in the game tree represents a possible situation in the game\n- Each endgame situation is assigned a value\n\nAs we go through the game tree, we keep track of two values,\n- Alpha: Minimum value the max player can get, initially set to -‚àû\n- Beta: Maximum value the min player can get, initially set to +‚àû","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Archives":{"title":"üóÑÔ∏è Archives","content":"","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Areas":{"title":"üî≠ Areas","content":"This space is designed to link to all major notes within the area. \n- [[Zettelkasten|ZETTELKASTEN]]\n- ","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Assumptions-of-Linear-Regression":{"title":"Assumptions of Linear Regression","content":"Anscombe's Quartet - A group of four data sets with nearly identical descriptive statistics, yet different distributions and graphs. These are prime examples of why it's important to be selective on when to use linear regression!\n\n![](https://matplotlib.org/stable/_images/sphx_glr_anscombe_001.png)\n\n  \nThe assumptions we make about a dataset when choosing to apply linear regression:\n1. Linearity\n\tThere is a linear relationship between y and x, such that it can be modelled by the equation $y = mx+b$\n2. Homoscedasticity\n\tEqual Variance: There shouldn't be a cone type shape (increasing or decreasing) because it shows a dependance on dependent variable\n3. Multivariate Normality\n\tNormality of error distribution. As we look at data points around regression line, we want to see a normal distribution.\n4. Independence of observations, which includes no autocorrelation\n\tThere really should be no pattern to data, as this indicates each variable is not truly independent\n5. Lack of multicollinearity\n\tWhenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation - it undermines the statistical significance of an independent variable.\n...and an extra check that there aren't outliers skewing the data.","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Avro":{"title":"Avro","content":"- Each record has a header that describes the structure of data it contains\n\t- Data is stored as binary information, and information in header is used to parse data and extract fields\n\t- Good format for compressing data, minimize storage and network bandwidth requirements\n\n**Evolution of data formats to Avro:**\nWith [[notes/Delimited Text Files|Delimited Text Files]], there are some disadvantages:\n- Data types need to be inferred, and not a guarantee\n- Parsing is tricky if data has commas\n- Column names might not be there\n\n[[Relational Databases|Relational Databases]]: add types (fixing the first disadvantage):\n```\nCREATE TABLE fruits {\n\tid      integer PRIMARY KEY,\n\tname    varchar(20)\n}\n```\nHowever, this data is flat i.e. stored in plain text format and this data is stored in databases, where this definition might be different across databases.\n\n[[JSON]]: a widely accepted format that can take any form and easily shared over network. \n- Still, there's no type definitions. \n- Repeated keys leads to bigger JSON objects\n- No comments, metadata, documentation\n\nWith Avro, the advantages are:\n- Data is fully typed, schema is defined, and compressed automatically\n- Documentation is embedded into schema\n- Safe schema evolution\n\nSimilar to [[Parquet]], [[ORC]]","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":["DataFundamentals"]},"/notes/BLOB":{"title":"BLOB","content":"*Binary Large Object*\n- All files are stored as binary data\n\t- Commonly used to store images, video, audio and app-specific documents\n","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":["DataFundamentals"]},"/notes/Braitenberg-vehicles":{"title":"Braitenberg vehicles","content":"Source: [Braitenberg Vehicle Wiki](https://en.wikipedia.org/wiki/Braitenberg_vehicle)\n\nThis was a thought experiment by Valentino Braitenberg.\n\nIn the simplest form of this experiment, a *Braitenberg vehicle* is a machine with a light sensor and a motor. The two are connected such that the motor turns on when the sensor detects light, and turns off when the sensor detects no light. So, this gives the effect that the vehicle seeks out light sources.\n\nHence, the sensor-motor wiring gives rise to interesting effects. Note, this seems intelligent (to be moving towards light source) but is completely decentralized.\n\n**Agents**\n- System that processes data, produces an output from input\n- Reflex agent - reacts to input, Memory agent - reacts to maybe past decisions","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Collectors-Fallacy":{"title":"Collector's Fallacy","content":"To know about something is not the same as knowing something.\n\nThe idea here is that we like collecting information, hoping that it will be useful down the way. However, this mindless collection of ideas is creating a fake sense of knowledge. We know the information but we don't actually break it down into our own ideas and words. \n\nAm I guilty of this? Possibly~ I noted that my review sheets during exams was helpful because I was taking the content and writing them down as possible questions. In a way, I'm breaking this down into how I actually was understanding the content. There is some deliberation in this method!\n\nA good way to get out of this is: knowledge cycle!\n- Read any text for an hour, and when the time limit is over, look back and see what you have learnt!\n","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":["NoteTaking","ZettelkastenGuide"]},"/notes/Data-Formats":{"title":"Data Formats","content":"\u003e [!tip] TLDR\n\u003e - Data can be structured, unstructured or aggregated\n\u003e\t- Structured data is data that can be modelled. For example, text, numbers can fit into data tables\n\u003e\t- Unstructured data is data that is hard to model due it's size or nature. So, audio, video or large text documents fit the bill\n\u003e\t- Aggregated data is when multiple data sources are combined into one set. This is then used to create a statistical report that makes inferences about the population.\n\nStructured data is data that adheres to a schema, most commonly, tabular representation\n- Columns represent the attributes of entity, Rows represent each instance of data entity\n- Relational model: Use key values to refer to entities in different databases\n\nSemi-structured data is information with some structure, but has variations between instances. [[JSON]] documents allows for representing this type of data (it's flexible)\n\nUnstructured data that has no structure.\n\nFormat is import for [[Data Stores|storing data]], retrieving for analysis and reporting data.","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Data-Stores":{"title":"Data Stores","content":"There are two broad categories of data store:\n- file stores\n- databases\n# File Stores\n*Store data in files*\n\nThe file format used to store data depends on:\n1. [[notes/Data Formats|Type of data to be stored]]\n2. Application/Services that need to interact (i.e. perform CRUD operations) with data\n3. Purpose of storage: Does this need to be readable by humans, or optimized for efficient storage and processing\n## Common File Formats\n- [[Delimited Text Files]]\n- [[JSON]]\n- [[XML]]\n- [[BLOB]]\n- [[Avro]]\n- [[ORC]]\n- [[Parquet]]\n# Databases\n*Central system where data is stored and searched through*\n- [[Relational Databases]]\n- [[Non-relational Databases]]","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":["DataFundamentals"]},"/notes/Delimited-Text-Files":{"title":"Delimited Text Files","content":"*Technology examples: CSV, TSV files*\n\n- Stored in plain text format with specific field delimiters and row terminators\n\t- In CSV (comma-separated values), field delimiters = comma and row terminators = new line\n\t\t- First line may contain title to columns\n\t- Also, TSV (tab-separated values) with only difference being field delimiters = tabs\n\t- Space-delimited where spaces and tabs are used to separate columns\n\t- Fixed-width data where each field has fixed number of characters\n\t\t- More detailed explanation here: https://www.softinterface.com/Convert-XLS/Features/Fixed-Width-Text-File-Definition.htm\n- Why use this?: human-readable format\n\n\n","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":["DataFundamentals"]},"/notes/Elementary-Graph-Algorithms":{"title":"Elementary Graph Algorithms.","content":"- A graph $G = (V, E)$ is composed of $V$, a set of vertices and $E \\in V \\times V$ \n- The two standard way of representing a graph is:\n\t- Adjacency List\n\t\t- Good idea when representing sparse graphs i.e. $|E| \u003c\u003c |V|^2$ where number of edges is a lot less than the possible connections between all vertices\n\t\t- Pros\n\t\t\t- Easy to transform into a weighted graph, given a weighted function\n\t\t\t- Amount of memory for both directed/undirected is $\\theta(V+E)$\n\t\t- Cons\n\t\t\t- No easy way to determine if an edge exists\n\t- Adjacency Matrix\n\t\t- Good idea when representing dense graphs i.e. $|E| \u003c |V|^2$ where number of edges is close to the most number of connections between all vertices\n\t\t- Pros\n\t\t\t- Easier to use then adjacency list, where undefined edges can be represented as 0, `null`, `undefined` etc.\n\t\t- Cons\n\t\t\t- Amount of memory for directed and undirected is $\\theta(V^2)$, can be optimized to lose diagonal if no self-loops\n\t\t\t\t- In undirected, we can lose half this space since $(u, v) = (v, u)$","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/Ethics-in-AI":{"title":"Ethics in AI","content":"\n**AI and Society**\n- The big question has always been¬†**will AI destroy jobs**?\n    - AI will replace the unpleasant or dangerous jobs, completing tasks with more efficiency and precision\n    - Rise of AI accelerates¬†_growth/consumption spiral_\n        - Companies want to lower prices, laying off workers to cut costs\n        - Lower prices is higher demand, so higher production (using more resources here)\n        - In modern countries, where population doesn't grow, individual person must consume more (marketing helps with this!)\n    - More people left unsatisfied (consuming goods they don't want, high unemployment etc)\n    - There needs to be an \"economy for the common good\" where we go for sustainability, rather than profit\n\n**Filter Bubbles**¬†The goal of content recommendation systems, especially those used in social media/newsfeeds, is to provide personalized searches. The unintended effect can be an¬†_intellectual isolation_¬†- you keep being offered curated results, but you find yourself in a bubble of only your own ideas.\n\n- How is this different from an epistemic bubble? In this situation, the user never gets the information by fault of the algorithm. Epistemic bubble is that the opposing view is simply left out.\n\n**Echo Chambers**¬†Stemming from filter bubbles, we see echo chambers: Users come across content that only amplifies \u0026 reinforces their beliefs. Essentially, it's confirmation bias.","lastmodified":"2024-03-07T04:10:22.311626992Z","tags":[]},"/notes/First-Order-Logic":{"title":"First-Order Logic","content":"G√∂del's Theorems i.e. [[../Math/G√∂del's Completeness Theorem|G√∂del's Completeness Theorem]] and [[../Math/G√∂del's Incompleteness Theorem|G√∂del's Incompleteness Theorem|]] were important to AI.\n\nFrom the completeness theorem, automatic theorem provers were created - In the 1950s, the Logic Theorist was made to show computers could process numbers and symbols.\n- LISP and PROLOG: programming languages created to process symbolic structure\n\nIn the incompleteness theorem, we know that there are true statements which can not be proved. This shows the limits of using a logic-based system. Another limit is the [[notes/artificial intelligence/Halting Problem|Halting Problem]] i.e. there is no program that can decide whether a program runs in an infinite loop or not.","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Function-Composition-in-Javascript":{"title":"Function Composition in JavaScript","content":"\nJavaScript is a multi-paradigm programming language - it has supports for several [[Programming Paradigm]], including [[Functional Programming]].\n\nFunction Piping vs. Composition\n- It's a matter of how functions are being executed: If functions are executed from left to right, that's a pipe while right to left is called compose\n\nNote: Functional Programming is about working with pure functions (functions with no side effects). However, that's not possible in real-life applications that deal with state and side-effects - that's where MONAD, a design pattern comes in. \n\nIn monadic composition, we add an extra requirement: It needs to explain how it got to the end result. Let's build a math expression\n```\n// The actual function attached with a description\nconst add6 = x =\u003e [x+5, '+6']\nconst multiply6 = x =\u003e [x*6, '*6']\n```","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Functional-Programming":{"title":"Functional Programming","content":"\n*What is Functional Programming?* A [[Programming Paradigm]].\n\nThe main focus is on \"what are we trying to solve\", rather than \"how are we trying to solve\" - It's a declarative type of programming\n- A nice way to imagine this is when jump into a cab, we tell the driver where we want to go. We don't give him turn-by-turn instructions i.e. imperative approach.\nIt is based on \u003cins\u003eLambda Calculus\u003c/ins\u003e: \n- Lambda Calculus is about studying computations with functions, which can also be used to simulate Turing Machine\n\t- In this logic system, functions are anonymous. We do not give them explicit names. $square\\_sum(x,y) = x^2 + y^2$ rewritten as $(x,y) \\rightarrow x^2 + y^2$    \n\t- Uses function of a single input, so the above example can be reworked:  $(x,y) \\rightarrow x^2 + y^2$  to $x \\rightarrow (y \\rightarrow x^2 + y^2)$. This is also now an example of a [[Higher-Order Functions]]","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/G%C3%B6dels-Completeness-Theorem":{"title":"G√∂del's Completeness Theorem","content":"\nFor any collection of first-order statements, every semantic implication of those statements is syntactically provable within first-order logic.\n\nSimply put,\n- all statements which are true in all models are provable\n\nFirst-order logic\n- Includes quantifiers e.g. $\\forall$, $\\exists$, predicates e.g. greater than, equal to and logical connectives e.g. and, or etc.\n- Allows relations between objects, definition of functions","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/G%C3%B6dels-Incompleteness-Theorem":{"title":"G√∂del's Incompleteness Theorem","content":"\nSource: [Numberphile on G√∂del's Incompleteness Theorem](https://www.youtube.com/watch?v=O4ndIDcDSGc)\n\nThe simple idea: There might be a conjecture that's true, but there is no way to prove it.\n- Why? There might be a gap between **truth** and **proof**.\n\nFirst, mathematics might not be consistent (i.e. existence of paradoxes). We set down axioms - statements that are self-evidently true for all cases in our mathematical system.\n- Before G√∂del, the initial idea was: We can prove any statement in mathematics if we have a strong enough set of axioms.\n\nG√∂del Coding - Any mathematical statement could be turned into a unique number. A little bit of a simplification but imagine that if a statement can be derived from an axiom, both the numbers will be divisible by each other.\n\nG√∂del's Challenge - Assume a statement cannot be proved from the set of axioms. This statement can be turned into a mathematical equation.\n- Assume that statement is false. This means the statement can be proved from the set of axioms.\n- Contradiction! We assumed this statement can not be proved, but now we have deduced it can be. Since we assume mathematics is consistent, this has to be false.\n\nSo, for any consistent mathematical system, there must be a statement that cannot be proved from the set of axioms.\n\nThe \"regressive\" solution is to add this as an axiom, but that's not really a good solution. I can still find out a statement that's not provable by the axioms.","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Higher-Order-Functions":{"title":"Higher-Order Functions","content":"\nA higher-order function is a function which takes a function as a parameter or which returns the function or both. In JavaScript, an example of this is setTimeout() where one of the parameters is a function we run after some time is passed. \n```\nsetTimeout(() =\u003e {\n\tconsole.log('')\n}, time)\n```","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Image-Feature-Vector":{"title":"Image Feature Vector","content":"A way to abstract information i.e. image, so that it's numerically quantifiable. Simply put, a list of numbers that represents an image. \n\nWhen building an image search engine, the first step is figure out what image descriptor to use. Am I trying to note a change in color, extract an object or differentiate between texture?\n- This will handle the logic needed to represent our image as numbers\n\nAssume we use raw pixel descriptor as our image descriptor.\n- Basically, take all the pixels in an image and without any kind of processing, represent an individual pixel by it's color values (RGB, each channel is 0-255) or intensity (0-255)\n\nA color mean and standard deviation descriptor would find the average and standard deviation of each channel of the image (so, each pixel's RGB values to form 3 numbers)\n- Helps find the spatial distribution of color in an image!\n\nA color histogram descriptor can also help tell us the distribution of colors in an image. If we divide the color space into discrete number of binds and then, count the number of pixels that fall into each bin, we have ourselves a color histogram.","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Image-Formats-Compression-Comparision":{"title":"Image Formats \u0026 Compression Comparision","content":"Digital graphic files will generally fall into two categories:\n1. Raster graphics are created using a grid of tiny pixels, making them very simple\n2. Vector graphics are creating using detailed paths of points \u0026 lines to render images. \n\nThe simplest format begins with a bitmap, .bmp extension. It's the truest image where it stores every pixel (i.e. Raster file type), it's lossless and uncompressed. \n\nJPEG, Joint Photographic Experts Group, solves the issue of large file sizes and are great for images with gradients. \n- 'Lossy' bitmap format: To maintain small file size, some information of the image is discarded. Saving at maximum quality maximizes as much image quality as possible.\n- JPEG Artefacts are result of an aggressive data compression result or conversion between different formats. This is especially noticeable in images with sharp lines, or sharp contrast between two colours. \n\nResource: [Mathematics behind JPEG](https://cuhkmath.wordpress.com/2012/10/06/mathematics-behind-jpeg/)\nThe Fourier coefficients indicate how the function oscillates. So, if a function is close to a constant value, the Fourier coefficients of that function decay very fast. So, just with the first few terms of the Fourier series, we will be close to the original function. \n\nHow can we use this property to store images efficiently in JPEG format?\n- Calculate the Fourier coefficients of an image (i.e. perform a 2D Fourier Transform on the image to represent in frequency domain).\nIn python, this can be achieved by:\n``` python\nimport numpy as np\nf_transform = np.fft.fft2(image)\n```\n- Store the first few big Fourier coefficients, discard the small, high-frequency coefficients\nIt's also why JPEG does not work well with storing text - there is a big change in background colour and text colour. These are big coefficients with high frequency, and discarding this information leads to JPEG artefacts.\n\nPNG, which contrary to JPEG, they render sharp contrasts better than gradients by being more smarter with how they store image. It's a raster file format, lossless and smaller file size. \n\nHow does it compress images? \nIt compares individual pixels with neighbouring pixels. If they are of the same color, it compresses this information.\n\nSVG, Scalable Vector Graphics, is a vector graphic file which stores the instructions on how to draw. These are impossible for photos, but nice for logos or simpler images.","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/JSON":{"title":"JSON","content":"*JavaScript Object Notation, hierarchical document schema to define objects with many attributes*\n\nBest demonstrated with an example:\n```json\n{\n\t\"fruits\": [\n\t\t{\n\t\t\t\"name\": \"apple\",\n\t\t\t\"colour\": [\"red\"]\n\t\t},\n\t\t{\n\t\t\t\"name\": \"orange\",\n\t\t\t\"colour\": [\"orange\"],\n\t\t\t\"taste\": \"interesting\"\n\t\t}\n\t]\n}\n```","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Multiple-Linear-Programming":{"title":"Multiple Linear Regression","content":"Statistical technique that uses two or more independent variables to predict the outcome of a dependent variable\n- determine the relative contribution of each independent variable in total variance\n\nEquation of Multiple Linear Regression:\n$y = b_0 + b_1X_1 + b_2X_2 + + b_3X_3 + ... + b_nX_n$\n- $y$ is the dependent variable, $b_0$ is the y-intercept (constant), and $X_n$ is the independent variable (with a slope coefficient)\n\n**Dummy Variables**\nFor categorical variables, how can we represent them in a multiple linear regression equation? We can use dummy variables - **variable that takes values of 0 and 1, where the values indicate the presence or absence of something**. For each level of categorical variable, we assign a dummy variable and use that in our equation.\n\n*Dummy Variable Trap*\n- Attributes that are highly correlated and one variable predicts the others.\n- Occurs with one-hot encoding of categorical data where one dummy variable predicts the other\n- Results in multicollinearity, affecting regression models\n- Solution: remove one dummy variable!\n","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Non-relational-Databases":{"title":"Non-relational Databases","content":"","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["DataFundamentals"]},"/notes/ORC":{"title":"ORC","content":"*Optimized Row Columnar Format, organized into columns that optimize read/write operations for Apache Hive*\n- Apache Hive: data warehouse that has fast data summarization and querying over large datasets\n- Columnar storage format to allow for optimized column-based operations like filtering/aggregation\n- ORC stores data in a series of stripes where each strip is a collection of rows. \n\t- each stripe is further divided into a series of data chunks where each stores data for columns\n\t- also, metadata can be used to quickly read data without scanning entire file\n\t- ORC can store indexes for specific columns, for faster retrieval of specific rows\n\nSimilar to [[Parquet]], [[Avro]]","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["DataFundamentals"]},"/notes/Parquet":{"title":"Parquet","content":"*Open-source columnar storage file format used for big data processing*\n- Columnar storage format to allow for optimized column-based operations like filtering/aggregation\n- Data in parquet file is divided into columns, and groups of columns are organized into row groups\n\t- each row group contains a section of data, and group of columns are organized into row groups\n\t- each row group has some data, and columns within a row group are stored together\n- Parquet files contains metadata that define structure of data. The three main metadata that can be found:\n\t- file metadata - high-level information such as schema, row groups etc.\n\t- column metadata - encoding, data types, compression etc\n\t- page-header metadata - data page, dictionary references etc.\n- Dictionary encoding: Compresses repetitive data by replacing those values with short id's that refer to a dictionary of unique values\n- Predicate pushdown: Query Optimization technique that filters data at source before it's read on memory\n\t- By applying filtering conditions early in the read process, only relevant data is loaded into memory\nSimilar to [[ORC]], [[Avro]]\n\nResource:\n- https://learncsdesigns.medium.com/understanding-apache-parquet-d722645cfe74","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["DataFundamentals"]},"/notes/Programming-Paradigm":{"title":"Programming Paradigm","content":"\n*Definition:* Methods or set of rules to structure code and solve a problem\n\nThe common types of paradigm:\n- Procedural Programming\n- Logical Programming\n- [[Functional Programming]]\n- Object-oriented Programming","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Projects":{"title":"‚úÖ Projects","content":"","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/Relational-Databases":{"title":"Relational Databases","content":"","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["DataFundamentals"]},"/notes/Resources":{"title":"üìö Resources","content":"","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":[]},"/notes/XML":{"title":"XML","content":"*Markup language that provides rules to define data*\n- tags in XML are markup symbols to define data\n\t- allows for data coding to get information flows across systems\n- XML schema (XSD): document that describes rules on structure of XML file\n- XML parser: process or read XML files to extract data, check syntax or validate against schema\n- Note: While similar in appearances, this is different from HTML\n\t- ability to define custom tags, as opposed to pre-defined HTML tags\n\t- HTML is for content, XML stores and transports data\n\t- XML is case-sensitive, HTML is not.","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["DataFundamentals"]},"/notes/Zettel":{"title":"Zettel","content":"Essentially, a single note on one particular topic! The anatomy of a zettel looks like:\n- Unique Identifier: an unambiguous address\n\t- This is mandatory! Why? Cause it allows for a unique way to identify each note. There are some ways we can go about achieving this!\n\t- Luhmann-ID\n\t\t- It's a hierarchy and sort of similar to how indexing with bullet points work. You have Note 1 and Note 2, and a note between this would be called Note 1a\n\t- Time-based ID + Arbit. unique string\n\t\t- This is what i'm going to use since Obsidian has support for this. Note, each [[Zettel]] will end up having it's own unique string i.e. \"title\" anyways.\n- Body/Content: the actual content\n- Footer/References: source of knowledge or personal thoughts\n\t- Also, consider [[../Collector's Fallacy]]\nWhen adding links to other content, there needs to be an explanation as to why it is there! If not, this is not knowledge but information!\n- This was an interesting point mentioned somewhere in: https://zettelkasten.de/introduction/#luhmann-s-zettelkasten. How do we categorize text as \"knowledge\" or \"information\"? Even better, how do I know what I am writing is knowledge (very critical for zettelkasten)!\n\t- Well, information is \"dead\" - it needs processing for it to give any relevant information. If I write out all the calories I consumed in a day, that's just information on by daily eating habits. Knowledge would be to dissect this and make some inferences. \n\nThere are a few things to consider in terms of actual contents: Writing the note, expanding the note, and when has it become too inflated. \n- Keep a draft outline which involves bulletting ideas \n- When expanding a note: add items to a collection, add references to back up a point or write a text to incorporate new knowledge. \n- There's no universal guideline to know when it's too big, but re-reading notes and considering if it can be broken down further.\n\nA great read: [Object Tags vs. Topic Tags](https://zettelkasten.de/posts/object-tags-vs-topic-tags/)","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["NoteTaking","ZettelkastenGuide"]},"/notes/Zettelkasten":{"title":"Zettelkasten","content":"Note: I'm referencing [zettelkasten.de](https://zettelkasten.de/posts/overview/#the-introduction-to-the-zettelkasten-method) to understand how to incorporate this into my daily note-taking!\n\nContrary to what a \"[[Zettel]]\" should be, i.e. adhere to principle of atomicity and be \"hyper-textual\", I am not placing any such constraints here. While I will try to achieve this, my main goal is to familiarize myself with how it works. \n\nIn this system, it is recommended that there are no categories as explained in this article: [#Why Categories for Your Note Archive are a Bad Idea](https://zettelkasten.de/posts/no-categories/)\n\nHowever, this isn't the most suitable for my style. I need a way to differentiate my personal items from work or school. So, let's keep them really general! For now, internally, the four major categories are:\n- **PERSONAL**: Related to health, finance, personal logs etc\n- **WORK**: Related to internships, resumes, etc\n- **STUDY**: Related to true knowledge items, things I'm learning currently\nI will try my best to avoid subcategories, but I can see situations where it's better to have order over an organic structure.\n\n\u003e [!tip] When starting out, ask yourself:\n\u003e  **Is your Zettelkasten designed for a single writing project or topic, or do you want to map all you know with this tool?**\n\nI want to map everything I know! I want to create a place where I can write about everything I'm learning and connect it to other concepts I've learned. This ecosystem will benefit from overview notes as gentle intros to a topic, similar to this note.ZettelkastenTools\n","lastmodified":"2024-03-07T04:10:22.315626984Z","tags":["NoteTaking","ZettelkastenGuide"]}}